{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fxM3H-O1TytL",
        "_MlLHJnhT1No",
        "9T_E2WNWUHs2",
        "00DsbxRJULtt",
        "u4_YpegMUSds",
        "3GxlVulBUWO0",
        "1SucH1uSUYzI",
        "0MdqvWWoUb73",
        "8wrr0jn_UkMk",
        "rdCvmSmnUoNt"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Specification gaming and goal misgeneralisation in grid worlds\n",
        "\n",
        "*Monday, October 13<sup>th</sup>.*\n",
        "\n",
        "*Lab by Matthew Farrugia-Roberts.*\n",
        "\n",
        "Welcome to the first lab for\n",
        "  [AI Safety and Alignment](https://robots.ox.ac.uk/~fazl/aisaa/),\n",
        "MT 2025.\n",
        "\n",
        "Structure of today's lab:\n",
        "\n",
        "0. Preliminaries---in which we install dependencies and introduce JAX.\n",
        "1. Agents and environments---in which we introduce two core elements of the\n",
        "   elements of reinforcement learning framework and explore a simple grid-world\n",
        "   environment.\n",
        "2. Reward functions---in which we introduce the third element of reinforcement\n",
        "   learning, and train an agent in our simple environment.\n",
        "3. Specification gaming---in which we explore the consequences of failing to\n",
        "   account for 'creative' ways of optimising our reward function.\n",
        "4. Generalisation---in which we train in a distribution of environments in the\n",
        "   hope that the agent will learn to behave correctly in situations it has\n",
        "   never seen.\n",
        "5. Goal misgeneralisation---in which we explore the consequences of failing to\n",
        "   account for ambiguity in the specification of our goals.\n",
        "\n",
        "The supporting code for today's lab can be found at\n",
        "  [github.com/matomatical/reward-lab](https://github.com/matomatical/reward-lab).\n",
        "\n",
        "If you get stuck, you can:\n",
        "\n",
        "* Ask your TAs,\n",
        "* Ask your peers, or\n",
        "* Check the solutions at the end of the notebook (use sparingly).\n"
      ],
      "metadata": {
        "id": "kUrauWZ6P508"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 0: Preliminaries\n",
        "====================\n",
        "\n",
        "Setting up the notebook and saying hello to JAX."
      ],
      "metadata": {
        "id": "so5W0NF1QfRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning the notebook\n",
        "--------------------\n",
        "\n",
        "**Before you start working in this notebook, you must create a copy in your own google drive folder. That way you will be able to modify the notebook and save your changes.**\n",
        "\n",
        "File > Save a copy in Drive"
      ],
      "metadata": {
        "id": "xQFqYwJnn-X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Choosing a runtime\n",
        "------------------\n",
        "\n",
        "For parts 1--3, the default CPU runtime is sufficient. When you reach parts 4\n",
        "and 5, it makes sense to switch over to a GPU (fast) or TPU (fastest) and\n",
        "repeat the installation and imports.\n"
      ],
      "metadata": {
        "id": "8em_0QN6Qwsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuring the runtime\n",
        "-----------------------\n",
        "\n",
        "Install custom code and dependencies.\n"
      ],
      "metadata": {
        "id": "pwB-0hSgQnph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/matomatical/reward-lab.git /content/reward-lab/ || git -C /content/reward-lab pull"
      ],
      "metadata": {
        "id": "3LM0ehDtQpSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002ad1b3-fc65-4e0a-9e87-be558a2e7044"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/reward-lab'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 97 (delta 57), reused 65 (delta 25), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (97/97), 60.60 KiB | 20.20 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/reward-lab\n",
        "!pip install --quiet -r requirements.txt\n",
        "!pip install --quiet -r requirements-notebook.txt"
      ],
      "metadata": {
        "id": "U3hX157Wb2dS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d30e4289-2c97-4fef-9a3b-12d12227f4a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/reward-lab\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m153.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Allow custom widgets in Colab.\n"
      ],
      "metadata": {
        "id": "UqZMoLCggmB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "_K_xpIEmgkj2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import dependencies.\n"
      ],
      "metadata": {
        "id": "2I18Swy2Q7c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import einops\n",
        "import optax\n",
        "import tqdm.notebook\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Callable\n",
        "from jaxtyping import Array, Float, Bool, PRNGKeyArray"
      ],
      "metadata": {
        "id": "MARod0hrQ8T8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import classes and functions for this lab\n",
        "(see [github.com/matomatical/reward-lab](https://github.com/matomatical/reward-lab) for source).\n"
      ],
      "metadata": {
        "id": "Zjb-QzCfRBsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import strux\n",
        "from util import display_rollout, display_rollouts, InteractivePlayer, LiveSubplots, display_envs\n",
        "from potteryshop import Item, State, Action, Environment, collect_rollout\n",
        "from evaluation import RewardFunction, compute_return, evaluate_behaviour\n",
        "from agent import ActorCriticNetwork\n",
        "from ppo import ppo_train_step, ppo_train_step_multienv"
      ],
      "metadata": {
        "id": "kewwQLSTRBRP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set some constants."
      ],
      "metadata": {
        "id": "siqHiDEmRFND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DISCOUNT_RATE = 0.995"
      ],
      "metadata": {
        "id": "d7l6bNCWRHJ-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hello, JAX!\n",
        "-----------\n",
        "\n",
        "This workshop uses [JAX](https://jax.readthedocs.io/), a Python deep learning\n",
        "framework that is a modern alternative to PyTorch and TensorFlow. Much modern\n",
        "reinforcement learning research, especially at Oxford, takes place in JAX,\n",
        "making it worth learning. However, today's activities don't require much\n",
        "familiarity with JAX. For our purposes, note the following:\n",
        "\n",
        "* JAX programs operate on JAX arrays, which are basically immutable NumPy\n",
        "  arrays. With a few exceptions, where you would write\n",
        "    `np.function(numpy_array)` or `numpy_array.method()`,\n",
        "  you can instead write\n",
        "    `jnp.function(jax_array)` or `jax_array.method()`.\n",
        "\n",
        "* Due to immutability, JAX has a slightly more verbose way of managing random\n",
        "  state compared to other frameworks. It is generally necessary to pass around\n",
        "  a `key` object corresponding to a node in a tree of random states, and to\n",
        "  manually advance the state.\n",
        "\n",
        "* One cool feature of JAX is that, once you write a function with JAX arrays,\n",
        "  you can use `jax.vmap(function)` to get a version that works with arrays that\n",
        "  have extra batch dimensions. This means we normally don't have to worry about\n",
        "  managing batch dimensions ourselves at all. We'll see `jax.vmap` a couple of\n",
        "  times throughout the code in this notebook.\n",
        "\n",
        "* Another cool feature of JAX is that, once you write a function with JAX\n",
        "  arrays, you can use `jax.jit(function)` to get a version that is just-in-time\n",
        "  compiled and optimised to run efficiently on your specific CPU, GPU, or TPU.\n",
        "  The resulting speed boost is one of the main selling points of JAX. We will\n",
        "  see `jax.jit` in action today, giving us faster feedback loops than if we had\n",
        "  written the notebook using PyTorch.\n",
        "\n",
        "* JAX's powerful function transformations like `jax.jit` and `jax.vmap` come at\n",
        "  a cost. In order for functions to be easily batchable and to run well on GPUs\n",
        "  and TPUs, the computational graph of each function needs to be made \"uniform\"\n",
        "  in that the shapes and operations involved don't depend on the values in the\n",
        "  input arrays. For example:\n",
        "\n",
        "  * We can't use `if` statements that depend on array values.\n",
        "  * We can't use `while` loops whose termination condition depends on array\n",
        "    values.\n",
        "  * We can use `for` loops with fixed numbers of iterations, but they are\n",
        "    unrolled in the computational graph causing long compile times.\n",
        "  * We can't use some NumPy operations, such as boolean indexing, that result\n",
        "    in arrays whose shapes depend on the values of other arrays.\n",
        "\n",
        "  JAX provides its own primitives and patterns for getting around these issues\n",
        "  and writing the kind of expressive Python programs we are used to.\n",
        "\n",
        "Throughout this notebook, there are examples and hints on the necessary JAX\n",
        "knowledge to get you through what limited use of JAX we need for the exercises,\n",
        "and you can always ask your tutors or an LLM for help if you are facing a weird\n",
        "JAX error.\n",
        "\n",
        "After today, if you are interested in learning more about JAX,\n",
        "  [see](https://github.com/n2cholas/awesome-jax)\n",
        "  [these](https://far.in.net/hijax)\n",
        "  [resources](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "saZJDj9ASf5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1: Agents and environments\n",
        "===============================\n",
        "\n",
        "In reinforcement learning (RL), we model a sequential decision-making task with\n",
        "two main entities:\n",
        "\n",
        "1. An **environment,** which models the state of the world, keeping track of\n",
        "   how the world changes in response to the actions of an agent (speaking of\n",
        "   which...)\n",
        "2. An **agent,** who repeatedly receives information about the current state of\n",
        "   the environment, chooses an action, and executes that action in the\n",
        "   environment.\n",
        "\n",
        "In this part, we'll familiarise ourselves with these concepts.\n",
        "\n",
        "Note: The framework of reinforcement learning would not be complete without\n",
        "discussing a third entity, the **reward function.** We'll come to reward\n",
        "functions in part 2.\n",
        "\n"
      ],
      "metadata": {
        "id": "3x4UtebYSkj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markov decision process formalism\n",
        "---------------------------------\n",
        "\n",
        "A popular formalism for defining environments is that of the **Markov decision\n",
        "process (MDP).** A (rewardless) MDP is a tuple\n",
        "  $(S, A, \\iota, \\tau)$\n",
        "where:\n",
        "\n",
        "* $S$ is a set of environment states,\n",
        "* $A$ is the set of actions available to the agent from any state,\n",
        "* $\\iota \\in \\Delta(S)$ is a distribution of initial states (the states the\n",
        "  environment starts in before the agent takes any actions), and\n",
        "* $\\tau : S \\times A \\to \\Delta(S)$ is a conditional transition distribution:\n",
        "  given the current state $s$ and agent action $a$, $\\tau(s,a)$ gives the\n",
        "  probability for the environment to transition into each possible next state.\n",
        "\n",
        "(Note: This definition omits the reward function and discount factor usually\n",
        "included in the definition of an MDP. We'll return to defining those in the\n",
        "next part.)\n",
        "\n",
        "The Markov decision process earns its name from the fact that the transitions\n",
        "as defined here satisfy a Markov property, whereby they are independent of the\n",
        "path taken to get to the state before the transition.\n",
        "\n",
        "An environment represented by an MDP is often paired with an agent represented\n",
        "by an action-selection **policy** of the form\n",
        "  $$\n",
        "    \\pi : S \\to \\Delta(A),\n",
        "  $$\n",
        "where $\\pi(s)$ represents the probability that the agent will take each\n",
        "possible action given the state $s$.\n",
        "Sometimes, the policy is defined to take as input only a certain subset of\n",
        "state information (called an *observation*), or perhaps a sequence of\n",
        "states (or observations), having the effect of imbuing the agent with a memory.\n"
      ],
      "metadata": {
        "id": "BTa_iXRkSntK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pottery shop: A simple environment\n",
        "----------------------------------\n",
        "\n",
        "Here is a picture of an environment called \"pottery shop\".\n",
        "\n",
        "![Pottery Shop](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMAAAADgCAMAAABirpkDAAAAAXNSR0IArs4c6QAAADxQTFRFAAAADStFIDxWVE5ojWl60IFZ/6pe/9Sj/+zWw4iOm05Ocjs+1e3tfKLCQmuhJEd90ct/laBoVHBkO05IOxWh3AAAABR0Uk5TAP////////////////////////+64WOpAAADfklEQVR4nO2b0Y6bMBBF0223D7ttqg3//6/FUkaaTm08wJgJyTlP4Av2nEiWwctePk/OJbuAvSCQzeVDURo+FnjEHIHsHIHsHIHsHIHsHIHs/PwCnycHgWzOL/AIE3GauVXw5AggECXwo8FRgqXIrxlbfC9HAIEIgVLo9wZWYqRAKUyK9OYIILBX4H2mJ/A+GFuc4MkRQCBKQC4qRcvFpe0oASnu644VaOUIIBApoCfJdKdkP2eWCtib27Frk7iVI4BAhIAsWC1GFK1z+bH0BLUCrRwBBKIE9MObR8Bynakde5kM3hwBBCIErIQW8Rb/a+bPnXK8R6LWbhexfyYxAggECNQkbEetxUkErndaAnsWvyJRW+QQQCBSYA9XQ1S/gkxk+8KPgIBAh+EChd93Ivss2OKbC9leEGiAgO5Ejm0+chLrRay6uevtBIGNPL+AZ/PVPkTp83J/mbzyQmMn8sjNXwQQiBDodVorXiZTOf82U4qWCVyOS9ueor2bvwggMEqgh+7kbcYuZKVta981JoO0I1COEXCwS8AuHtImx293WudR2OILCBQQcNIV6G2uWgnL6I9CajkCowvs5c8lcEYQyAaBbBDIBoFsEMjmOQQyHsKicgSy8+cQeLSi1uQIZOfPK3AWEMgGgSPQm2p2oxmBI3gdgUdZnGrYDz6kHYGj8tcQeGRkIg/9+HskCGRzagFb/GlXYgSyCBGQTuR4bb4HvYgN+3odgc7Yzy3Qe8iqfeihz3v53oc8+yD33yRGAIHk/x/o5VuK1rl87Dfs63UEENiALXptvpbJIO0IbM3X8toCN4Vu8+ae4m4VPDkCCEQK9DZXbcdrck//dpHy5AggECmQifwA+oXdkyMQBQKZ2OLsRF3KEYgAgWz0IqVf3D05AhEgkP2Rh31Qq03iVo4AAhECGUV7N297OQIIjBI4msngzRGI4rUFSoNdJPQkOqL4pfGXcgQQiBCQi2qLiGbk4tYbv5UjgECkQAnsC/VR9MZv5QhE8doCNlyayEcUb8dfyhFAIEpAQv3ivKUIOV5779L4SzkCCEQL1CaRZ3PW3qPPh2/uIoBAgIBcpC9cGnTNx+G9+3vjL+UIIBAhoC8Slor34PkRNL3xWzkCCEQJ6Iu2Fmsn8tp+euPXcgQKCDjH7wrs3ZztTdwRm8MICAhszB/mD91bQSCb0wv8BRxcWtxDcvJIAAAAAElFTkSuQmCC)\n",
        "\n",
        "Pottery shop is an example of a grid-world environment, where everything plays\n",
        "out on a finite grid of positions (in this case, a 6 by 6 grid).\n",
        "\n",
        "This grid world contains various objects:\n",
        "\n",
        "* Urns---the products of the pottery shop.\n",
        "* Shards---some urns have been broken, leaving behind piles of shards.\n",
        "* A bin---there is a bin in the corner that stores shards.\n",
        "* A robot---there is a small blue robot who can move around the grid, pick up\n",
        "  shards, carry them around, and drop them (e.g., into the bin).\n",
        "  If the robot crashes into one of the urns, the urn will break, creating a new\n",
        "  pile of shards.\n",
        "\n",
        "The pottery shop environment is implemented in the source file\n",
        "`environment.py`. Some relevant snippets of code are as follows.\n",
        "\n",
        "### Environment layout\n",
        "\n",
        "Pottery shop actually refers to a family of environments of different sizes and\n",
        "with different initial configurations of objects. We represent such an\n",
        "environment as an `Environment` dataclass as follows:\n",
        "\n",
        "```python\n",
        "@strux.struct\n",
        "class Environment:\n",
        "    init_robot_pos: UInt8[Array, \"2\"]\n",
        "    init_items_map: UInt8[Array, \"world_size world_size\"]\n",
        "    bin_pos: UInt8[Array, \"2\"]\n",
        "```\n",
        "\n",
        "In particular, the fields are as follows:\n",
        "\n",
        "* `init_robot_pos` contains the row and column grid coordinates of the spawn\n",
        "  position of the robot.\n",
        "* `bin_pos` similarly contains the row and column grid coordinates of the spawn\n",
        "  position of the bin.\n",
        "* `init_items_map` is an N by N array where N is the size of the grid world.\n",
        "  The contents of the array map to the presence of shards or urns in the\n",
        "  respective grid squares.\n",
        "\n",
        "The following enumeration type explains how to interpret the numbers in\n",
        "`init_items_map`.\n",
        "\n",
        "```python\n",
        "class Item(enum.IntEnum):\n",
        "    EMPTY = 0\n",
        "    SHARDS = 1\n",
        "    URN = 2\n",
        "```\n",
        "\n",
        "The coordinates in `init_robot_pos` and `bin_pos` should range from `0` to\n",
        "`world_size-1`, and the values in `init_items_map` should all be `0`, `1`, or\n",
        "`2`.\n",
        "\n",
        "> ### `jaxtyping`\n",
        ">\n",
        "> This code snippet uses type annotations of the form `UInt8[Array, \"shape\"]`.\n",
        "> `UInt8` and `Array` were imported from `jaxtyping` above (along with some\n",
        "> other array types, namely `Float` and `Bool`). These type annotations\n",
        "> describe the intended type and shape of the JAX arrays used in this code:\n",
        ">\n",
        "> * `init_robot_pos` and `bin_pos` are JAX arrays of unsigned 8-bit integers\n",
        ">   with shape `(2,)`.\n",
        "> * `init_items_map` is a JAX array of unsigned 8-bit integers with shape\n",
        ">   `(world_size, world_size)`.\n",
        ">\n",
        "> Some other examples:\n",
        ">\n",
        "> * `Float[Array, \"height width 3\"]` indicates an array of floats with shape\n",
        ">   `(height, width, 3)`, for example for representing some RGB image data.\n",
        "> * `Bool[Array, \"\"]` indicates a zero-dimensional array of booleans, that is,\n",
        ">   a boolean scalar.\n",
        ">\n",
        "> These annotations are not currently type checked, and should be treated like\n",
        "> comments (in particular, they may contain typos). Nevertheless, we hope they\n",
        "> array-manipulating code easier to follow. Feel free to include these\n",
        "> annotations in your own code, too.\n",
        "\n",
        "> ### `@strux.struct`\n",
        ">\n",
        "> This code snippet uses the `strux.struct` wrapper, defined in `strux.py`.\n",
        "> Basically, this is equivalent to the built-in Python wrapper\n",
        "> `dataclasses.dataclass(frozen=True)`, but with some extra functionality to\n",
        "> make the dataclasses integrate nicely with JAX function transformations.\n",
        "\n",
        "\n",
        "### Environment state\n",
        "\n",
        "The environment object encodes the initial configuration of the pottery shop.\n",
        "But once we start taking actions, the state will change. At any time, the\n",
        "current state of the grid world is represented by the following dataclass.\n",
        "\n",
        "```python\n",
        "@strux.struct\n",
        "class State:\n",
        "    robot_pos: UInt8[Array, \"2\"]\n",
        "    bin_pos: UInt8[Array, \"2\"]\n",
        "    items_map: UInt8[Array, \"world_size world_size\"]\n",
        "    inventory: UInt8[Array, \"\"]\n",
        "```\n",
        "\n",
        "The fields `robot_pos`, `bin_pos`, and `items_map` are the dynamic versions of\n",
        "`init_robot_pos`, `bin_pos`, and `init_items_map` from the environment\n",
        "configuration.\n",
        "\n",
        "What's new is `inventory`, an integer scalar that represents what kind of item\n",
        "the robot is carrying. This is initially `Item.EMPTY`, but changes to\n",
        "`Item.SHARDS` if the robot picks up a pile of shards (and changes back to\n",
        "`Item.EMPTY` if the robot drops the shards).\n",
        "\n",
        "### Agent actions\n",
        "\n",
        "The agent is responsible for controlling the robot as it moves around the grid.\n",
        "In each interaction, the agent sees the current state, and then chooses what\n",
        "the robot should do from the following options.\n",
        "\n",
        "```python\n",
        "class Action(enum.IntEnum):\n",
        "    WAIT = 0 # do nothing\n",
        "    UP = 1 # move up\n",
        "    LEFT = 2 # move left\n",
        "    DOWN = 3 # move down\n",
        "    RIGHT = 4 # move right\n",
        "    PICKUP = 5 # pick up item\n",
        "    PUTDOWN = 6 # drop held item\n",
        "```\n",
        "\n",
        "### Environment methods\n",
        "    \n",
        "So much for defining the data types involved, the actual implementation of the\n",
        "environment logic takes place inside the methods of the environment class.\n",
        "The two most important methods are the following:\n",
        "\n",
        "* `def reset(self: Environment) -> State`: Initialises a `State`.\n",
        "\n",
        "* `def step(self: Environment, state: State, action: Action) -> State`: Takes\n",
        "  the current state and the agent's actions and returns the resulting state of\n",
        "  the environment (for example, moving the robot, updating its inventory,\n",
        "  smashing urns).\n",
        "\n"
      ],
      "metadata": {
        "id": "82A271BNSoKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Explore pottery shop\n",
        "----------------------------\n",
        "\n",
        "Your first task is simply to instantiate and explore a pottery shop\n",
        "environment. Complete the following sub-tasks:\n",
        "\n",
        "1. Create an `Environment` object, including a world size of at least 4, at\n",
        "   least one pile of shards, and at least one urn.\n",
        "\n",
        "2. Interact with the environment using the built-in simulator\n",
        "  `InteractivePlayer`.\n",
        "\n",
        "  \n",
        "Note: Sub-task 1 involves specifying JAX arrays for each of the arguments of\n",
        "the `Environment` constructor. To create a JAX array, you can use\n",
        "`jnp.array(...)` where the input is a list, a nested list, or a numpy array.\n",
        "Other numpy-like methods for creating arrays should work as well, such as\n",
        "`jnp.zeros`, `jnp.ones`, `jnp.arange` etc.).\n"
      ],
      "metadata": {
        "id": "hGb7Bj8DSoTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = # TODO"
      ],
      "metadata": {
        "id": "Q4oFzpaVTLcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "InteractivePlayer(env)"
      ],
      "metadata": {
        "id": "0xXQOhfFTOGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus task 1: Framing questions\n",
        "-------------------------------\n",
        "\n",
        "If you have time and interest, consider the following questions:\n",
        "\n",
        "1. Could you write down the pottery shop environment in the MDP formalism? How\n",
        "   would this compare to the Python implementation? What definitions or methods\n",
        "   correspond to the set of states, the set of actions, the initial state\n",
        "   distribution, and the transition distribution?\n",
        "\n",
        "2. Consider the joint system of you and your computer, while you are clicking\n",
        "   the buttons that control the pottery shop robot. Decompose this system into\n",
        "   an environment and an agent. What are the states, and what are the actions?\n",
        "\n",
        "3. Is the division between agent and environment that you came up with for the\n",
        "   previous question unique? Are there other ways you could divide up the\n",
        "   system into an environment and an agent? How many can you think of?\n",
        "\n",
        "4. When you interacted with the environment, were you acting out a memoryless\n",
        "   policy, or would representing your policy require histories of environment\n",
        "   states as inputs? Does it depend on where you draw the boundary between the\n",
        "   agent and the environment?\n",
        "\n"
      ],
      "metadata": {
        "id": "mLKmeEMISoa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: Reward functions\n",
        "========================\n",
        "\n",
        "We now return to our description of the reinforcement learning framework.\n"
      ],
      "metadata": {
        "id": "gaVWb2keSodQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reward functions\n",
        "----------------\n",
        "\n",
        "Once we have an environment with a set of states $S$ and an agent with a set of\n",
        "actions $A$, a **reward function** is defined as a function\n",
        "$$\n",
        "  r : S \\times A \\times S \\to \\mathbb{R}\n",
        "$$\n",
        "that maps individual interactions between the environment and the agent to\n",
        "scalar reward values.\n",
        "\n",
        "Note that here the input triple $(s,a,s') \\in S \\times A \\times S$ represents\n",
        "an environment state ($s$), the agent's choice of action ($a \\sim \\pi(s)$), and\n",
        "the resulting successor state ($s' \\sim \\tau(s,a)$).\n",
        "Sometimes reward functions are defined on states alone ($r: S \\to \\mathbb{R}$),\n",
        "state-action pairs ($r:S \\times A \\to \\mathbb{R}$), or state pairs ($r : S\n",
        "\\times S \\to \\mathbb{R}$), but the most general form includes the above three\n",
        "elements.\n"
      ],
      "metadata": {
        "id": "xcto0MQvSof3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximising expected return\n",
        "--------------------------\n",
        "\n",
        "Given a reward function and a **trajectory** (or **rollout**) of states and actions,\n",
        "$$\n",
        "  s_0, a_0, s_1, a_1, \\ldots\n",
        "$$\n",
        "we define the **return** $R$ as the discounted cumulative sum of rewards:\n",
        "$$\n",
        "  R(s_0, a_0, \\ldots) = \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t, s_{t+1})\n",
        "$$\n",
        "where $\\gamma \\in (0,1)$ is a discount factor that controls how much more to\n",
        "value rewards received earlier versus later in time.\n",
        "\n",
        "A typical formulation of the goal of reinforcement learning algorithms is,\n",
        "given an environment, a reward function, and a discount factor, find a policy\n",
        "that **maximises expected return** (taking the expectation over the\n",
        "stochasticity in the initial state distribution, the transition distribution,\n",
        "and the policy itself).\n"
      ],
      "metadata": {
        "id": "CFE2AnfdSoiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reward hypothesis\n",
        "---------------------\n",
        "\n",
        "Reward functions play a central role in the discipline of reinforcement\n",
        "learning. Their centrality is driven by the following hypothesis, called the\n",
        "**reward hypothesis:**\n",
        "\n",
        "> \"That all of what we mean by goals and purposes can be well thought of as\n",
        "> maximization of the expected value of the cumulative sum of a received scalar\n",
        "> signal (reward).\"\n",
        ">\n",
        "> ---Richard Sutton, [The reward hypothesis](http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html)\n",
        "\n",
        "The reward hypothesis essentially claims that, regardless of what kind of\n",
        "purposes we want a system to fulfil, we can sensibly formulate that behaviour\n",
        "as the maximum expected return for *some* reward function. Once we find the\n",
        "right reward function, then we just need to apply a reinforcement learning\n",
        "algorithm to find a policy with the behaviour we desire.\n",
        "\n",
        "While there is some debate in the field as to the extent to which the reward\n",
        "hypothesis is true in its maximal form (that maximisation of scalar returns\n",
        "covers *all* goals and purposes), it is certainly true that:\n",
        "\n",
        "* There are some AI problems where finding the right reward function has\n",
        "  allowed us to specify behaviours that were infeasible to specify imperatively\n",
        "  (e.g. by programming, as in classical software engineering) or even\n",
        "  declaratively (e.g. by generating labelled examples, as in supervised\n",
        "  learning), with the most notable example being computer Go playing and recent\n",
        "  examples of frontier reasoning models.\n",
        "\n",
        "* This possibility has driven a substantial portion of interest in the field of\n",
        "  reinforcement learning, with some seeing reinforcement learning as a key\n",
        "  component of the likely path to human-level and super-human AI in the future.\n",
        "\n",
        "Of course, it remains to find the right reward function!\n"
      ],
      "metadata": {
        "id": "KGcOhxH6Sokj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Interpreting a reward function\n",
        "--------------------------------------\n",
        "\n",
        "Your second task is to study a reward function and consider the behaviour it\n",
        "incentivises. Here is the reward function:\n"
      ],
      "metadata": {
        "id": "cd4KNrbVSonK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reward1(state: State, action: Action, next_state: State) -> float:\n",
        "    item_below_robot = state.items_map[\n",
        "        state.robot_pos[0],\n",
        "        state.robot_pos[1],\n",
        "    ]\n",
        "    pickup_reward = (\n",
        "        (item_below_robot == Item.SHARDS)\n",
        "        & (state.inventory == Item.EMPTY)\n",
        "        & (action == Action.PICKUP)\n",
        "    ).astype(float)\n",
        "    dispose_reward = (\n",
        "        (state.bin_pos[0] == state.robot_pos[0])\n",
        "        & (state.bin_pos[1] == state.robot_pos[1])\n",
        "        & (state.inventory == Item.SHARDS)\n",
        "        & (action == Action.PUTDOWN)\n",
        "    ).astype(float)\n",
        "    total_reward = pickup_reward + dispose_reward\n",
        "    return total_reward"
      ],
      "metadata": {
        "id": "vU9eNWF0WHoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that `&` represents elementwise 'and' for (NumPy or) JAX arrays.\n",
        "\n",
        "Your task is to answer the following questions:\n",
        "\n",
        "1. Describe the kinds of transitions for which this reward function returns\n",
        "   `1.0` vs `0.0`.\n",
        "\n",
        "2. What kinds of qualitative behaviours do you think a reward designer who came\n",
        "   up with this reward function is trying to incentivise?\n",
        "\n",
        "3. What kinds of qualitative behaviours maximise return subject to this reward\n",
        "   function? Are they the same as the previous answer?\n"
      ],
      "metadata": {
        "id": "TUj-58xPSopf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement learning\n",
        "----------------------\n",
        "\n",
        "Next, let's apply a reinforcement learning algorithm to see what behaviours the\n",
        "agent learns given this reward function."
      ],
      "metadata": {
        "id": "HibZMSJeSor1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The library `ppo.py` provides a function `ppo_train_step` that collects some\n",
        "rollouts and trains an agent network on these using a reinforcement learning\n",
        "algorithm (a simplified form of proximal policy optimisation). Here is a function that wraps this\n",
        "into a training loop:\n"
      ],
      "metadata": {
        "id": "0sAF1OoiSouL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent(\n",
        "    key: PRNGKeyArray,\n",
        "    env: Environment,\n",
        "    net: ActorCriticNetwork,\n",
        "    reward_fn: RewardFunction,\n",
        "    num_train_steps: int = 512,\n",
        "    num_train_steps_per_vis: int = 8,\n",
        ") -> ActorCriticNetwork:\n",
        "    learning_rate = 0.001\n",
        "    max_grad_norm = 0.5\n",
        "    optimiser = optax.chain(\n",
        "        optax.clip_by_global_norm(max_grad_norm),\n",
        "        optax.adam(learning_rate=learning_rate),\n",
        "    )\n",
        "    optimiser_state = optimiser.init(net)\n",
        "\n",
        "    liveplot = LiveSubplots(['return'], num_train_steps)\n",
        "    for t in tqdm.notebook.trange(num_train_steps):\n",
        "        key_step, key = jax.random.split(key)\n",
        "        net, optimiser_state, metrics = ppo_train_step(\n",
        "            key=key_step,\n",
        "            net=net,\n",
        "            env=env,\n",
        "            reward_fn=reward_fn,\n",
        "            optimiser=optimiser,\n",
        "            optimiser_state=optimiser_state,\n",
        "            # ppo step hyperparameters\n",
        "            num_rollouts=16,\n",
        "            num_env_steps=64,\n",
        "            discount_rate=DISCOUNT_RATE,\n",
        "            eligibility_rate=0.95,\n",
        "            proximity_eps=0.1,\n",
        "            critic_coeff=0.5,\n",
        "            entropy_coeff=0.001,\n",
        "        )\n",
        "        liveplot.log(t, {'return': metrics['return']})\n",
        "        if (t+1) % num_train_steps_per_vis == 0:\n",
        "            liveplot.refresh()\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "lwpJvf4NWXH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning up shop\n",
        "----------------\n",
        "\n",
        "Let's use this training loop to train a policy.\n",
        "\n",
        "1. We'll need a neural network parametrisation of a policy we can train by\n",
        "gradient descent. The following code defines a small CNN-based policy network.\n",
        "\n",
        "2. We'll then call the training function with this network, your manually-instantiated environment from task 1, and the above reward function.\n"
      ],
      "metadata": {
        "id": "DizE4OCkWgUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = jax.random.key(seed=42)\n",
        "key_init, key = jax.random.split(key)\n",
        "net1 = ActorCriticNetwork.init(\n",
        "    key=key_init,\n",
        "    obs_height=env.world_size,\n",
        "    obs_width=env.world_size,\n",
        "    net_channels=8,\n",
        "    net_width=16,\n",
        "    num_conv_layers=2,\n",
        "    num_dense_layers=1,\n",
        "    num_actions=len(Action),\n",
        ")\n",
        "\n",
        "key_train, key = jax.random.split(key)\n",
        "net1 = train_agent(\n",
        "    key=key_train,\n",
        "    net=net1,\n",
        "    env=env,\n",
        "    reward_fn=reward1,\n",
        "    num_train_steps=256,\n",
        ")\n"
      ],
      "metadata": {
        "id": "AwzyYr4NWirC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### `jax.random.key` and `jax.random.split`\n",
        ">\n",
        "> The above code is our first use of JAX's quirky PRNG state management\n",
        "> pattern.\n",
        ">\n",
        "> In other frameworks, the first step to drawing reproducible random samples\n",
        "> would be to initialise a PRNG object, to be passed into each random function.\n",
        "> Each of these functions would draw samples from the PRNG whenever needed,\n",
        "> mutating the internal state of the PRNG so that every additional sample is\n",
        "> independent.\n",
        ">\n",
        "> Mutability complicates the kinds of function transformations JAX specialises\n",
        "> in. So in JAX, we need a way to manage a PRNG's state without mutating an\n",
        "> object. The JAX solution is essentially for the user to explicitly create a\n",
        "> copy of the PRNG (with a modified state) and pass different parts of the\n",
        "> state around to each function. Actually, instead of advancing the state\n",
        "> linearly, it's more accurate to say that we *split* the state into multiple\n",
        "> independent child states. Each function gets its own independent branch of\n",
        "> the phylogenetic tree of PRNG states, which it can continue to split and pass\n",
        "> around to its liking.\n",
        ">\n",
        "> We see this in the above examples. First, `key = jax.random.key(seed=42)`\n",
        "> initialises a root random state. Then,\n",
        ">   `key_init, key = jax.random.split(key)`\n",
        "> forks it into two children:\n",
        ">\n",
        "> * `key_init`---a branch for passing into the network initialisation function.\n",
        "> * `key` (again)---since we don't need the root state after we have split it,\n",
        ">   it is idiomatic to reassign one of the children to the old variable if we\n",
        ">   want to split again later in the current scope.\n",
        ">\n",
        "> In the next cell, we repeat this pattern, sending `key_train` into the\n",
        "> training function, and reassigning `key` to a fresh child for subsequent use.\n",
        ">\n",
        "> One thing to watch out for when using JAX in notebooks is that the JAX idiom\n",
        "> of reassigning to `key` and the notebook idiom of repeated execution of cells\n",
        "> do not mix well. This is fine for today, but for robust reproducibility, one\n",
        "> should either use fresh names for each child key or have each cell restart\n",
        "> from its own seed.\n"
      ],
      "metadata": {
        "id": "f1ylKinGSowh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Interpreting an agent's behaviour\n",
        "-----------------------------------------\n",
        "\n",
        "The next task is to study the behaviour of the agent we learned, and see to\n",
        "what extent it matches the behaviour we intended or expected when we designed\n",
        "this reward function. The following code samples and animates some trajectories\n",
        "from the learned network. Your task is to run the code and study the policies,\n",
        "discerning if there are any difference between:\n",
        "\n",
        "1. The behaviour the designer of the reward function likely intended;\n",
        "2. The behaviour you expected after studying the reward function; and\n",
        "3. The actual behaviour observed.\n"
      ],
      "metadata": {
        "id": "rfMeUXR9Soy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key_rollout = jax.random.key(seed=1)\n",
        "rollout = collect_rollout(\n",
        "    env=env,\n",
        "    key=key_rollout,\n",
        "    policy_fn=net1.policy,\n",
        "    num_steps=64,\n",
        ")\n",
        "display_rollout(env, rollout)\n"
      ],
      "metadata": {
        "id": "75Um4eJ_XBUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Change the seed above to see trajectories with different random results when\n",
        "sampling actions from the policy.\n",
        "\n",
        "Questions:\n",
        "\n",
        "1. What qualitative behaviours do you observe?\n",
        "\n",
        "2. Are there discrepancies between the behaviour you observe and the\n",
        "   intended/expected behaviour? If so, list them.\n"
      ],
      "metadata": {
        "id": "zrt88lyLSo1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3: Specification gaming\n",
        "============================\n",
        "\n",
        "**Note: Finish task 3 before reading further.**\n"
      ],
      "metadata": {
        "id": "SZuNSHP4So31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all has gone to plan, you should be looking at an example of specification\n",
        "gaming (also known as reward hacking), in which the reinforcement learning\n",
        "algorithm found a better way to increase expected return than the class of\n",
        "valid solutions the reward designer had in mind.\n",
        "\n",
        "The two kinds of discrepancies we expect to see in this example are:\n",
        "\n",
        "1. The agent prefers to repeatedly pick up and drop shards compared to picking\n",
        "   them up once and taking them to the bin; and\n",
        "2. The agent is willing to break urns to find new shards to pick up.\n",
        "\n",
        "In this part, we will redesign the reward function to prevent these unintended\n",
        "behaviours from being incentivised.\n"
      ],
      "metadata": {
        "id": "-i9kVHD1XQc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inverse reward hypothesis\n",
        "-----------------------------\n",
        "\n",
        "Recall the reward hypothesis from above. While this hypothesis is usually\n",
        "invoked in the context of designing an agent using reinforcement learning by\n",
        "identifying an appropriate reward, it applies more broadly as stated.\n",
        "\n",
        "In particular, the reward hypothesis applies to the behaviour of existing\n",
        "agents. Suppose we have a system whose behaviour fulfils some purpose. Then (by\n",
        "the reward hypothesis), that behaviour can be well thought of as maximising\n",
        "expected return for *some* reward function.\n",
        "\n",
        "One could call the above corollary the **inverse reward hypothesis,** following\n",
        "  [Stuart Russell (1998)](https://dl.acm.org/doi/10.1145/279943.279964), also\n",
        "  [Andrew Ng and Stuart Russell (2000)](https://dl.acm.org/doi/10.5555/645529.657801),\n",
        "who introduced the problem of *inverse reinforcement learning* (that of\n",
        "taking a policy and extracting from it a reward function for which that policy\n",
        "maximises expected return).\n"
      ],
      "metadata": {
        "id": "UMPv3DLcSo6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Reward functions as behavioural probes\n",
        "----------------------------------------------\n",
        "\n",
        "We won't trouble ourselves with inverse reinforcement learning algorithms\n",
        "today. Instead, let's just take a step in this direction by quantifying the\n",
        "misbehaviour of our agents using reward functions that incentivise each\n",
        "problematic behaviour. We won't use these reward functions for training, but we\n",
        "can use them to measure to what extent we are inadvertently training for these\n",
        "behaviours---as 'behavioural probes'.\n",
        "\n",
        "Your next task is to write one reward function that measures the extent to\n",
        "which an agent is engaging in each problematic behaviour:\n",
        "\n",
        "1. First, write a reward function `reward_drop` that assigns `+1` every time\n",
        "   the agent drops a shard (other than in the bin).\n",
        "2. Second, write a reward function `reward_break` that assigns `+1` every time\n",
        "   the agent breaks an urn.\n"
      ],
      "metadata": {
        "id": "5eDUYJyXSo8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_drop(state: State, action: Action, next_state: State) -> float:\n",
        "    # TODO\n",
        "    pass"
      ],
      "metadata": {
        "id": "6P6SJyP6XaOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_break(state: State, action: Action, next_state: State) -> float:\n",
        "    # TODO\n",
        "    pass"
      ],
      "metadata": {
        "id": "Bi-tItgSXafu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantifying reward hacking\n",
        "--------------------------\n",
        "\n",
        "Having written these reward functions, we can get a quantitative signal about\n",
        "whether our policy is engaging in these misbehaviours. We can collect a batch\n",
        "of trajectories and then score them according to each reward function and plot\n",
        "the result, using the following convenience function:\n",
        "\n",
        "```python\n",
        "def evaluate_behaviour(\n",
        "    env: Environment,\n",
        "    net: ActorCriticNetwork,\n",
        "    key: PRNGKeyArray,\n",
        "    reward_fn: RewardFunction,\n",
        "    num_steps: int = 64,\n",
        "    num_rollouts: int = 1000,\n",
        "    discount_rate: float = 0.995,\n",
        ") -> Float[Array, \"num_rollouts\"]\n",
        "```\n",
        "\n",
        "Note that, unlike a reward function that correctly incentivises the *intended*\n",
        "behaviour, we ideally want these reward functions *to be minimised.* Moreover,\n",
        "note that to tell if a reward function is minimised or maximised, we need to\n",
        "consider the range of possible returns, which depends on the reward function\n",
        "(in these simple environments it can be derived analytically).\n"
      ],
      "metadata": {
        "id": "KclBnHFPSo-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_fns = [reward1, reward_drop, reward_break]\n",
        "return_vecs = [\n",
        "    evaluate_behaviour(\n",
        "        key=jax.random.key(seed=1),\n",
        "        env=env,\n",
        "        net=net1,\n",
        "        reward_fn=r,\n",
        "    )\n",
        "    for r in reward_fns\n",
        "]"
      ],
      "metadata": {
        "id": "7X4KTuRpXrY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(len(reward_fns), figsize=(5,3*len(reward_fns)))\n",
        "for (reward_fn, returns, ax) in zip(reward_fns, return_vecs, axes):\n",
        "    ax.hist(returns)\n",
        "    ax.set_title(reward_fn.__name__)\n",
        "    ax.set_xlabel(\"return\")\n",
        "fig.tight_layout()\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "ncfgMmFDXwKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Potential shaping: Avoiding cycles\n",
        "----------------------------------\n",
        "\n",
        "Regarding the first problematic behaviour (repeatedly picking up and dropping\n",
        "shards), one solution would be to remove the reward for picking up shards\n",
        "altogether. The intention is for the shards to end up in the bin, not in the\n",
        "inventory for their own sake, anyway.\n",
        "\n",
        "However, sometimes rewarding instrumental goals can assist the agent's learning\n",
        "process. When we know that something is instrumentally useful for achieving a\n",
        "task, it would be nice if we could incorporate that knowledge into the reward.\n",
        "\n",
        "The only problem is that incorporating these kinds of hints into the reward is\n",
        "a delicate operation---as we have seen, it can lead to misspecification and\n",
        "reward hacking if it becomes possible to satisfy the hint without completing\n",
        "the original task!\n",
        "\n",
        "Fortunately, there is a sure-fire scheme for adding a so-called **shaping**\n",
        "term to a reward function without introducing such loops. This is a method\n",
        "called **potential shaping,** and it works as follows:\n",
        "\n",
        "1. Formulate the information as a (bounded) function of states, $\\Phi : S \\to\n",
        "   \\mathbb{R}$, called a potential function.\n",
        "2. When we transition from state $s$ to state $s'$, add reward $\\gamma\n",
        "   \\Phi(s')$ to represent gaining the potential from being in state $s'$, but\n",
        "   also *subtract* reward $\\Phi(s)$ to represent *losing* the potential from\n",
        "   *leaving* state $s$.\n",
        "3. This helps the agent learn to steer towards states with 'high potential',\n",
        "   without giving it a long-term incentive to stay there for the sake of this\n",
        "   potential---the potential should eventually either be lost (if the agent\n",
        "   leaves those states without achieving return) or actualised (if the policy\n",
        "   leaves the states and gains actual reward).\n"
      ],
      "metadata": {
        "id": "ikjRLZNsX0Rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus task 2: Cancelling potentials\n",
        "-----------------------------------\n",
        "\n",
        "Those of you who are theoretically inclined may like to try this optional\n",
        "exercise.\n",
        "\n",
        "Let\n",
        "  $\\Phi : S \\to \\mathbb{R}$ be a bounded potential function,\n",
        "  $r : S \\times A \\times S \\to \\mathbb{R}$ a bounded reward function, and\n",
        "  $\\gamma \\in (0,1)$ a discount rate.\n",
        "Define a shaped reward function\n",
        "  $r' : S \\times A \\times S \\to \\mathbb{R}$\n",
        "such that for all triples $s,a,s'$ we have\n",
        "$$\n",
        "  r'(s,a,s') = r(s,a,s') + \\gamma\\Phi(s') - \\Phi(s).\n",
        "$$\n",
        "\n",
        "Given a trajectory $s_0, a_0, s_1, a_1, \\ldots$, calculate the return under the\n",
        "two reward functions $r$ and $r'$ and show that they differ by an additive\n",
        "constant that depends only on $s_0$.\n",
        "\n",
        "Conclude that ordering on policies induced by the expected return under $r$ is\n",
        "the same as the ordering on policies induced by the expected return under\n",
        "$r'$.\n"
      ],
      "metadata": {
        "id": "zfUJzMZDSpBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5: Implementing potential shaping\n",
        "--------------------------------------\n",
        "\n",
        "Using potential shaping, write a reward function `reward_shaped` that still\n",
        "incentivises the robot to pick up shards but not to put them back down again.\n",
        "\n",
        "Notes:\n",
        "\n",
        "* Hint: Consider a potential function that looks at the contents of the\n",
        "inventory.\n",
        "* Like the original reward function, this reward function should also give a reward for putting the shards into the bin.\n",
        "* Moreover, to get a clear training signal, consider making the reward for putting shards into the bin larger than any potential lost from this action."
      ],
      "metadata": {
        "id": "s6JltZJ8SpDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_shaped(state: State, action: Action, next_state: State) -> float:\n",
        "    # TODO\n",
        "    pass"
      ],
      "metadata": {
        "id": "lVxcpcIyX8rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disincentivising specific behaviours\n",
        "------------------------------------\n",
        "\n",
        "Let's turn to the second problematic behaviour: breaking urns to find more\n",
        "shards. There are a couple of different approaches to disincentivising this\n",
        "behaviour. We'll use a simple approach of directly penalising transitions in\n",
        "which the robot breaks an urn.\n",
        "\n",
        "The main question is, how much of a negative reward should we assign for\n",
        "breaking an urn?\n",
        "\n",
        "* We need to set the negative reward large enough so that the agent is better\n",
        "  off *not* breaking the urn rather than breaking the urn and binning the\n",
        "  resulting shards.\n",
        "* We can't set the negative reward to be too large, or it will cause training\n",
        "  instability.\n",
        "\n",
        "You can try different values, but a good default value would be -2 reward for\n",
        "breaking an urn. The agent can recover 1 reward later if it bins the shards\n",
        "created by breaking the urn (the gain in return will be slightly less than 1\n",
        "due to discounting). Penalising -2 means the agent is clearly better off not\n",
        "breaking the urn.\n"
      ],
      "metadata": {
        "id": "FDgp91DjSpGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus task 3: Everyone has a price\n",
        "----------------------------------\n",
        "\n",
        "If you have time, consider the following two questions:\n",
        "\n",
        "1. If the reward for breaking an urn is -2, there are still situations in which\n",
        "   breaking an urn can lead to higher return than not breaking an urn.\n",
        "   Can you find any such situations?\n",
        "\n",
        "2. Can you think of any approach to defining a reward function that would make\n",
        "   breaking urns always suboptimal?\n"
      ],
      "metadata": {
        "id": "syD8gIkjSpIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 6: Penalty\n",
        "---------------\n",
        "\n",
        "Your task is to implement a reward function that assigns a negative reward to\n",
        "transitions in which the robot breaks an urn. Hint: You already implemented a\n",
        "related function in task 4.\n"
      ],
      "metadata": {
        "id": "1U9v8ZYYSpLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_no_break(state: State, action: Action, next_state: State) -> float:\n",
        "    # TODO\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "TmKzwnKlYL0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 7: Fixing the specification\n",
        "--------------------------------\n",
        "\n",
        "Your next task is to bring together the previous three tasks to eliminate\n",
        "specification gaming:\n",
        "\n",
        "1. Combine the rewards from tasks 5 and 6 into a single new reward function,\n",
        "   `reward2.`\n",
        "\n",
        "2. Train a new network, using the same environment, agent architecture, and\n",
        "   hyper-parameters as last time, but this time using the new reward function.\n",
        "\n",
        "3. Inspect some rollouts, manually and/or by using your evaluation reward\n",
        "   function probes, to confirm that the agent now behaves as intended.\n"
      ],
      "metadata": {
        "id": "FaUm1pI2SpNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reward2(state: State, action: Action, next_state: State) -> float:\n",
        "    # TODO\n",
        "    pass"
      ],
      "metadata": {
        "id": "LrgTEHJGYTiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (see part 1 section 'cleaning up shop' for a starting point)\n"
      ],
      "metadata": {
        "id": "5ku_5jIP00sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: inpsect some rollouts like in tasks 3 and 4"
      ],
      "metadata": {
        "id": "_xjhUveX1ReX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything goes to plan, you should see the agent learning to actually clean up at least one pile of shards!\n",
        "\n",
        "With more careful architecture selection and hyperparameter tuning, and more training, the agent could potentially do better---it could get more reward by cleaning up multiple shards.\n",
        "\n",
        "However, this is enough for today."
      ],
      "metadata": {
        "id": "rjNBfK61701I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4: Generalisation\n",
        "======================\n",
        "\n",
        "So far, we have worked with a specific grid-world layout. In practice, we want\n",
        "our agents to be able navigate and complete tasks in the complex environments,\n",
        "up to and including the real world.\n",
        "\n",
        "Real-world environments are almost infinitely complex---an agent will almost\n",
        "never see the same situation more than once. This means we could never hope to\n",
        "give our agents direct experience in every possible state they might encounter\n",
        "prior to deployment.\n",
        "\n",
        "Instead, we need to train agents that will **generalise** from the situations\n",
        "they encountered during training to the new situations they will face after\n",
        "deployment.\n",
        "\n",
        "In this part, we'll investigate a more complex version of our pottery shop\n",
        "environment where the same agent might face different shop layouts.\n"
      ],
      "metadata": {
        "id": "Cp5vtdW8SpPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generalisation in RL\n",
        "--------------------\n",
        "\n",
        "Generalisation is a foundational concept in machine learning. In reinforcement\n",
        "learning, generalisation is essentially no different:\n",
        "\n",
        "* We want a function. In this case, it's a function called a 'policy' and it\n",
        "  maps from states/observations to action probabilities.\n",
        "\n",
        "* We use deep learning to learn that function from data. The details here are a\n",
        "  little different in reinforcement learning than in supervised learning, but\n",
        "  at the end of the day, we are still using a gradient descent algorithm to\n",
        "  find weights that optimise some objective. In this case, the objective is\n",
        "  derived from maximising expected return.\n",
        "\n",
        "* We haven't explored all possible inputs to the function. There are some\n",
        "  states for which the policy's outputs have never been queried and subject to\n",
        "  calibration through the objective.\n",
        "\n",
        "Generalisation in reinforcement learning refers to how the policy responds to\n",
        "unseen states, particularly whether the action probabilities that it outputs\n",
        "for these states are consistent with the training objective of maximising\n",
        "expected return.\n"
      ],
      "metadata": {
        "id": "jW_N31ijSpSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 8: How does your policy generalise?\n",
        "----------------------------------------\n",
        "\n",
        "Let's see how your policy generalises:\n",
        "\n",
        "1. Design a new instance of the pottery shop environments where the robot and\n",
        "   items spawn in new positions. The world size should be the same as before,\n",
        "   as the network architecture assumes this shape.\n",
        "2. Without training in this new environment, generate and plot some rollouts\n",
        "   from your previously-trained agent `net2` in this new environment.\n",
        "3. Inspect the behaviour of the agent and qualitatively describe it.\n"
      ],
      "metadata": {
        "id": "F1-LJ_neSpU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env2 = # TODO"
      ],
      "metadata": {
        "id": "9Cse7wGeYeZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: display rollout like in task 3"
      ],
      "metadata": {
        "id": "6fizA67080Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procedurally generated environments\n",
        "-----------------------------------\n",
        "\n",
        "In most cases, training a policy in a fixed environment will cause the agent to\n",
        "learn a brittle policy that relies on many assumptions about the environment\n",
        "that happen to hold in the fixed environment.\n",
        "\n",
        "A common approach to learning more robust policies is to train the agent in a\n",
        "broad distribution of *procedurally generated* environments, all of which share\n",
        "some commonality (e.g. same world size) but do not allow the agent to rely on\n",
        "spurious assumptions (e.g. robot and item positions).\n",
        "\n",
        "The first step to such a training approach is to write some code to randomly\n",
        "generate variations of the environment. Take a look at the following example.\n"
      ],
      "metadata": {
        "id": "ExbaSJXMSpXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    key: PRNGKeyArray,\n",
        "    world_size: int,\n",
        "    num_shards: int,\n",
        "    num_urns: int,\n",
        ") -> Environment:\n",
        "    # place the bin in the top left corner of the world\n",
        "    bin_pos = jnp.zeros((2,), dtype=jnp.uint8)\n",
        "\n",
        "    # list of possible item/robot coordinates\n",
        "    coords = einops.rearrange(\n",
        "        jnp.indices((world_size, world_size), dtype=jnp.uint8),\n",
        "        'c h w -> c (h w)',\n",
        "    )\n",
        "    # exclude (0,0) (used for bin)\n",
        "    coords = coords[:, 1:]\n",
        "\n",
        "    # sample robot and item positions without replacement\n",
        "    num_positions = 1 + num_shards + num_urns\n",
        "    all_positions = jax.random.choice(\n",
        "        key=key,\n",
        "        a=coords,\n",
        "        shape=(num_positions,),\n",
        "        axis=1,\n",
        "        replace=False,\n",
        "    )\n",
        "    robot_pos = all_positions[:, 0]\n",
        "    items_pos = all_positions[:, 1:]\n",
        "\n",
        "    # create item map\n",
        "    items_map = jnp.zeros((world_size, world_size), dtype=jnp.uint8)\n",
        "    items_map = items_map.at[\n",
        "        items_pos[0, :num_shards],\n",
        "        items_pos[1, :num_shards],\n",
        "    ].set(Item.SHARDS)\n",
        "    items_map = items_map.at[\n",
        "        items_pos[0, num_shards:],\n",
        "        items_pos[1, num_shards:],\n",
        "    ].set(Item.URN)\n",
        "\n",
        "    return Environment(\n",
        "        init_robot_pos=robot_pos,\n",
        "        init_items_map=items_map,\n",
        "        bin_pos=bin_pos,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "qOTWGkG_YiAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use this code to generate a sample of environments as follows.\n"
      ],
      "metadata": {
        "id": "cEMyQisESpZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key_generate = jax.random.key(seed=1)\n",
        "num_envs = 32\n",
        "envs = jax.vmap(\n",
        "    generate,\n",
        "    in_axes=(0, None, None, None),\n",
        ")(\n",
        "    jax.random.split(key, num_envs),\n",
        "    6,\n",
        "    3,\n",
        "    4,\n",
        ")\n",
        "display_envs(envs, grid_width=8)\n"
      ],
      "metadata": {
        "id": "9v9zsFGmYmtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### `jax.vmap`\n",
        ">\n",
        "> This is our first use of automatic vectorisation with JAX. It's not necessary\n",
        "> to understand the details right now, but suffice it to say that this call to\n",
        "> `jax.vmap(generate, in_axes=(0, None, None, None))` transforms `generate`\n",
        "> from a function that takes a single key and returns a single environment to a function that takes an array of keys and returns an array of environments.\n",
        "> If you are interested to learn more, take a look at the JAX tutorial on this\n",
        "> topic:\n",
        ">\n",
        "> * [Automatic vectorization](https://docs.jax.dev/en/latest/automatic-vectorization.html).\n"
      ],
      "metadata": {
        "id": "Kj6FLgxASpcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training on a distribution of environments\n",
        "------------------------------------------\n",
        "\n",
        "Once we have a distribution of environments, we can integrate it into a\n",
        "reinforcement learning algorithm by, for example, sampling a new environment\n",
        "every time we want to reset the environment to collect a new rollout. Here is a\n",
        "modified version of `train_agent` that takes a procedural environment generator\n",
        "rather than a single environment.\n"
      ],
      "metadata": {
        "id": "dIor4PzOSpeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent_multienv(\n",
        "    key: PRNGKeyArray,\n",
        "    gen: Callable[[PRNGKeyArray], Environment],\n",
        "    net: ActorCriticNetwork,\n",
        "    reward_fn: RewardFunction,\n",
        "    num_train_steps: int = 512,\n",
        "    num_train_steps_per_vis: int = 8,\n",
        ") -> ActorCriticNetwork:\n",
        "    learning_rate = 0.001\n",
        "    max_grad_norm = 0.5\n",
        "    optimiser = optax.chain(\n",
        "        optax.clip_by_global_norm(max_grad_norm),\n",
        "        optax.adam(learning_rate=learning_rate),\n",
        "    )\n",
        "    optimiser_state = optimiser.init(net)\n",
        "\n",
        "    liveplot = LiveSubplots(['return'], num_train_steps)\n",
        "    for t in tqdm.notebook.trange(num_train_steps):\n",
        "        key_envs, key = jax.random.split(key)\n",
        "        envs = jax.vmap(gen)(jax.random.split(key_envs, 32))\n",
        "        key_step, key = jax.random.split(key)\n",
        "        net, optimiser_state, metrics = ppo_train_step_multienv(\n",
        "            key=key_step,\n",
        "            net=net,\n",
        "            envs=envs,\n",
        "            reward_fn=reward_fn,\n",
        "            optimiser=optimiser,\n",
        "            optimiser_state=optimiser_state,\n",
        "            # ppo step hyperparameters\n",
        "            num_env_steps=64,\n",
        "            discount_rate=DISCOUNT_RATE,\n",
        "            eligibility_rate=0.95,\n",
        "            proximity_eps=0.1,\n",
        "            critic_coeff=0.5,\n",
        "            entropy_coeff=0.01, # needs more exploration\n",
        "        )\n",
        "        liveplot.log(t, {'return': metrics['return']})\n",
        "        if (t+1) % num_train_steps_per_vis == 0:\n",
        "            liveplot.refresh()\n",
        "\n",
        "    return net\n"
      ],
      "metadata": {
        "id": "Upfmj2reYt_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning to solve a distribution of environments is more challenging than\n",
        "learning an individual environment, so we'll need a slightly larger policy\n",
        "and a longer training time.\n",
        "\n",
        "**[Accordingly, this is a good time to switch over to a GPU or TPU runtime, if\n",
        "you haven't yet.]**"
      ],
      "metadata": {
        "id": "mm-uWQEpYwAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "world_size = 4\n",
        "\n",
        "key = jax.random.key(seed=1)\n",
        "\n",
        "key_init, key = jax.random.split(key)\n",
        "net3 = ActorCriticNetwork.init(\n",
        "    key=key_init,\n",
        "    obs_height=world_size,\n",
        "    obs_width=world_size,\n",
        "    net_channels=16,\n",
        "    net_width=64,\n",
        "    num_conv_layers=5,\n",
        "    num_dense_layers=2,\n",
        "    num_actions=len(Action),\n",
        ")\n",
        "\n",
        "key_train, key = jax.random.split(key)\n",
        "net3 = train_agent_multienv(\n",
        "    key=key_train,\n",
        "    net=net3,\n",
        "    gen=functools.partial(\n",
        "        generate,\n",
        "        world_size=world_size,\n",
        "        num_shards=4,\n",
        "        num_urns=2,\n",
        "    ),\n",
        "    reward_fn=reward2,\n",
        "    num_train_steps=4096,\n",
        "    num_train_steps_per_vis=128,\n",
        ")\n"
      ],
      "metadata": {
        "id": "-S-K_8xsY7b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 9: How does the policy generalise now?\n",
        "-------------------------------------------\n",
        "\n",
        "Once `net3` finishes training, your task is to explore its generalisation\n",
        "properties:\n",
        "\n",
        "1. Design some different shop layouts to probe the agent's generalisation\n",
        "   properties.\n",
        "\n",
        "2. For each shop layout you design, predict how you think the agent will behave.\n",
        "\n",
        "3. Then manually inspect the agent's behaviour using `collect_rollout` and\n",
        "   `display_rollout`.\n",
        "\n",
        "4. Qualitatively characterise the kinds of environments where the agent\n",
        "   generalises correctly and the ones where it does not.\n"
      ],
      "metadata": {
        "id": "_H-WaX9EYv7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n"
      ],
      "metadata": {
        "id": "8fN5sP0iCBqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 5: Goal misgeneralisation\n",
        "==============================\n",
        "\n",
        "> **Note: Finish task 9 before reading further.**"
      ],
      "metadata": {
        "id": "YdRayBv8Yv4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all has gone to plan, you should have found at least one example of goal\n",
        "misgeneralisation. Let's move forward with the following example. When the\n",
        "policy is tested on environment layouts where the bin is outside of the corner,\n",
        "we expect to see the following:\n",
        "\n",
        "1. the policy generalises in terms of its ability to pick up shards, carry them\n",
        "   to a destination, and drop them there; but\n",
        "2. the policy fails to generalise in terms of its behavioural goal of carrying\n",
        "   the shards to the location of the bin.\n",
        "\n",
        "In this part, we will unpack this example and explore the effect of the\n",
        "procedural environment generator on the way the policy generalises its goal.\n"
      ],
      "metadata": {
        "id": "m8qykR6aYv23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 10: Elements of goal misgeneralisation\n",
        "-------------------------------------------\n",
        "\n",
        "Recall the informal definition of goal misgeneralisation from Langosco et al.\n",
        "(2022):\n",
        "\n",
        "> A deep RL agent is trained to maximize reward $R$... Assume that the agent is\n",
        "> deployed under distributional shift; that is, an aspect of the environment\n",
        "> (and therefore the distribution of observations) changes at test time. Goal\n",
        "> misgeneralization occurs if the agent now achieves low reward in the new\n",
        "> environment because it continues to act capably yet appears to optimize a\n",
        "> different reward $R' \\neq R$. We call $R$ the intended objective and $R'$ the\n",
        "> behavioral objective of the agent.\n",
        "\n",
        "Your next task is to line up the elements of this definition with our case of\n",
        "goal misgeneralisation:\n",
        "\n",
        "1. What is the distribution shift? Provide an example of an environment from\n",
        "   the test distribution (you can use one of the environments you designed for\n",
        "   the previous task).\n",
        "2. What is the behavioural objective in this case? Identify it and then write a\n",
        "   reward function `proxy` that encodes this behaviour.\n",
        "3. Use `evaluate_behaviour` and the code from the end of task 4 to show that\n",
        "   the policy achieves low return under the training reward function but high\n",
        "   return under the behavioural objective in `env_shift`.\n"
      ],
      "metadata": {
        "id": "jQQryTz-Yvzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_shift = # TODO"
      ],
      "metadata": {
        "id": "Z1CSwbsuZMiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def proxy(state: State, action: Action, next_state: State) -> float:\n",
        "    # TODO\n",
        "    pass"
      ],
      "metadata": {
        "id": "1EZCCYvrZOFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: histograms like in task 4"
      ],
      "metadata": {
        "id": "Q-IPdn4qZQhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: We call the behavioural objective `proxy` because it's correlated with\n",
        "the training reward function on the training environment distribution.\n"
      ],
      "metadata": {
        "id": "2TNbBQEdYvtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 11: Distribution shift\n",
        "---------------------------\n",
        "\n",
        "In tasks 9 and 10, you manually generated environment layouts in which the\n",
        "policy misgeneralises.\n",
        "\n",
        "Your next task is to automate the construction of these environments by writing\n",
        "a new procedural environment generator that samples from a broader distribution\n",
        "of environments.\n",
        "\n",
        "In particular, write a function `generate_shift`, a modification of `generate`\n",
        "from above, that randomises not only the item and robot spawn locations, but\n",
        "also the bin location.\n"
      ],
      "metadata": {
        "id": "OFyGT6QBYvnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_shift(\n",
        "    key: PRNGKeyArray,\n",
        "    world_size: int,\n",
        "    num_shards: int,\n",
        "    num_urns: int,\n",
        ") -> Environment:\n",
        "    # TODO\n",
        "    return Environment(\n",
        "        init_robot_pos=robot_pos,\n",
        "        init_items_map=items_map,\n",
        "        bin_pos=bin_pos,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "rWLqVC_wZXsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You can use this code to test your generator:\n"
      ],
      "metadata": {
        "id": "ii0jtAJ3Yvfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key_generate = jax.random.key(seed=1)\n",
        "num_envs = 32\n",
        "envs = jax.vmap(\n",
        "    generate_shift,\n",
        "    in_axes=(0, None, None, None),\n",
        ")(\n",
        "    jax.random.split(key, num_envs),\n",
        "    6,\n",
        "    3,\n",
        "    4,\n",
        ")\n",
        "display_envs(envs, grid_width=8)\n"
      ],
      "metadata": {
        "id": "bxh4Er7MZepn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training out of distribution\n",
        "----------------------------\n",
        "\n",
        "In principle, an easy way to fix goal misgeneralisation is to train a policy in\n",
        "a broader distribution of levels, like that generated by `generate_shift` as\n",
        "opposed to `generate`. If we train a new policy using this new environment\n",
        "generator, we should see goal misgeneralisation decrease:\n"
      ],
      "metadata": {
        "id": "G1G2KfMdYvPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "world_size = 4\n",
        "\n",
        "key = jax.random.key(seed=1)\n",
        "\n",
        "key_init, key = jax.random.split(key)\n",
        "net4 = ActorCriticNetwork.init(\n",
        "    key=key_init,\n",
        "    obs_height=world_size,\n",
        "    obs_width=world_size,\n",
        "    net_channels=16,\n",
        "    net_width=64,\n",
        "    num_conv_layers=5,\n",
        "    num_dense_layers=2,\n",
        "    num_actions=len(Action),\n",
        ")\n",
        "\n",
        "key_train, key = jax.random.split(key)\n",
        "net4 = train_agent_multienv(\n",
        "    key=key_train,\n",
        "    net=net4,\n",
        "    gen=functools.partial(\n",
        "        generate_shift,\n",
        "        world_size=world_size,\n",
        "        num_shards=4,\n",
        "        num_urns=2,\n",
        "    ),\n",
        "    reward_fn=reward2,\n",
        "    num_train_steps=4096,\n",
        "    num_train_steps_per_vis=128,\n",
        ")\n"
      ],
      "metadata": {
        "id": "dPrxOPIWZjbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_fns = [reward2, proxy]\n",
        "return_vecs = [\n",
        "    evaluate_behaviour(\n",
        "        key=jax.random.key(seed=1),\n",
        "        env=env_shift,\n",
        "        net=net4,\n",
        "        reward_fn=r,\n",
        "    )\n",
        "    for r in reward_fns\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(len(reward_fns), figsize=(5,3*len(reward_fns)))\n",
        "for (reward_fn, returns, ax) in zip(reward_fns, return_vecs, axes):\n",
        "    ax.hist(returns, bins=50)\n",
        "    ax.set_title(reward_fn.__name__)\n",
        "    ax.set_xlabel(\"return\")\n",
        "fig.tight_layout()\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "hcuP7_WSZmSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mitigating goal misgeneralisation in practice\n",
        "---------------------------------------------\n",
        "\n",
        "In practice, we might want to prevent goal misgeneralisation in a situation\n",
        "where:\n",
        "\n",
        "1. We only have access only to a given distribution of environments analogous\n",
        "   to `generate`;\n",
        "2. We face some unknown distribution shift in deployment, analogous to\n",
        "   evaluating in environments sampled from `generate_shift`; but\n",
        "3. *We don't have access to `generate_shift` during training!*\n",
        "\n",
        "In this case, the simple solution of just training in environments from\n",
        "`generate_shift` might not be available. Finding ways to mitigate this kind of\n",
        "goal misgeneralisation is an active research area.\n"
      ],
      "metadata": {
        "id": "isSdYZi8Z2PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "==========\n",
        "\n",
        "Well done, you have reached the end of this lab's tasks. Hopefully you have\n",
        "gained an appreciation for the relationship between a designers intention, a\n",
        "reward function, and a policy's behaviour in reinforcement learning:\n",
        "\n",
        "* In training environments, if there are behaviours that score higher return\n",
        "  than the designer's intended behaviours according to the reward function,\n",
        "  then the policy might learn to reward hack.\n",
        "* In out-of-distribution environments, the behaviour of the policy is not\n",
        "  necessarily determined by what the reward function *would have*\n",
        "  incentivised---rather it comes down to the inductive biases of the agent\n",
        "  architecture.\n",
        "\n",
        "Throughout the remainder of this module, you will see various reflections of\n",
        "this conceptual pattern playing out in different learning settings.\n"
      ],
      "metadata": {
        "id": "TclEC60YZ4cL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solutions\n",
        "=========\n"
      ],
      "metadata": {
        "id": "9WBCRQ8KTuWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 1 solution\n",
        "---------------"
      ],
      "metadata": {
        "id": "fxM3H-O1TytL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Of course, many environment layouts are permissible. Here is the one depicted\n",
        "above:\n",
        "\n",
        "```python\n",
        "env = Environment(\n",
        "    init_robot_pos=jnp.array((1,2), dtype=jnp.uint8),\n",
        "    init_items_map=jnp.array((\n",
        "      (0,0,0,0,2,2),\n",
        "      (0,1,0,0,0,2),\n",
        "      (0,0,0,0,0,0),\n",
        "      (0,1,1,0,0,2),\n",
        "      (0,0,0,0,2,2),\n",
        "      (2,0,1,0,2,2),\n",
        "    ), dtype=jnp.uint8),\n",
        "    bin_pos=jnp.array((0,0), dtype=jnp.uint8),\n",
        ")\n",
        "```\n"
      ],
      "metadata": {
        "id": "_8VZL98ZT8wl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 2 solution\n",
        "---------------\n"
      ],
      "metadata": {
        "id": "_MlLHJnhT1No"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Answers to questions:\n",
        "\n",
        "1. The reward function assigns reward 1 to transitions in which the robot picks\n",
        "   up a pile of shards, and transitions in which it drops a pile of shards\n",
        "   into the bin.\n",
        "\n",
        "2. The reward designer is probably trying to incentivise the agent to operate\n",
        "   the robot to pick up shards and put them into the bin, 'cleaning up' the\n",
        "   pottery shop.\n",
        "\n",
        "3. See below...\n"
      ],
      "metadata": {
        "id": "WRxByi60UCV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 3 solution\n",
        "---------------\n",
        "\n",
        "See opening of part 3.\n"
      ],
      "metadata": {
        "id": "R1IK0lJET6Ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 4 solution\n",
        "---------------\n"
      ],
      "metadata": {
        "id": "9T_E2WNWUHs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "There are a couple of ways to implement each of these, since the information\n",
        "is redundantly represented throughout the state, action, and next state.\n",
        "\n",
        "Here is a solution for the first function based on checking `state` and\n",
        "`action`, similar to `reward1`'s `pickup_reward`.\n",
        "\n",
        "```python\n",
        "def reward_drop(state: State, action: Action, next_state: State) -> float:\n",
        "    item_below_robot = state.items_map[\n",
        "        state.robot_pos[0],\n",
        "        state.robot_pos[1],\n",
        "    ]\n",
        "    return (\n",
        "        (state.robot_pos != state.bin_pos).any()\n",
        "        & (item_below_robot == Item.EMPTY)\n",
        "        & (state.inventory == Item.SHARDS)\n",
        "        & (action == Action.PUTDOWN)\n",
        "    ).astype(float)\n",
        "```\n",
        "\n",
        "Here is a solution for the second function that checks if shards under the\n",
        "robot's destination position were originally an urn.\n",
        "\n",
        "```python\n",
        "def reward_break(state: State, action: Action, next_state: State) -> float:\n",
        "    item_below_robot_after_transition = next_state.items_map[\n",
        "        next_state.robot_pos[0],\n",
        "        next_state.robot_pos[1],\n",
        "    ]\n",
        "    item_there_before_transition = state.items_map[\n",
        "        next_state.robot_pos[0],\n",
        "        next_state.robot_pos[1],\n",
        "    ]\n",
        "    return (\n",
        "        (item_below_robot_after_transition == Item.SHARDS)\n",
        "        & (item_there_before_transition == Item.URN)\n",
        "    ).astype(float)\n",
        "```\n"
      ],
      "metadata": {
        "id": "_NCgcmb-UJAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Bonus task 2 solution\n",
        "---------------------\n"
      ],
      "metadata": {
        "id": "00DsbxRJULtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $R$ denote the return with respect to reward function $r$, and $R'$ the\n",
        "return with respect to reward function $r'$. Then:\n",
        "\n",
        "\\begin{align*}\n",
        "  R'(s_0, a_0, \\ldots)\n",
        "  &= \\sum_{t=0}^\\infty \\gamma^t r'(s_t, a_t, s_{t+1})\n",
        "\\\\\n",
        "  &= \\sum_{t=0}^\\infty \\gamma^t (\n",
        "      r(s_t, a_t, s_{t+1}) + \\gamma\\Phi(s_{t+1}) - \\Phi(s_t)\n",
        "  )\n",
        "\\\\\n",
        "  &= \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t, s_{t+1})\n",
        "  + \\sum_{t=0}^\\infty \\gamma^{t+1} \\Phi(s_{t+1})\n",
        "  - \\sum_{t=0}^\\infty \\gamma^t \\Phi(s_t)\n",
        "\\\\\n",
        "  &= R(s_0, a_0, \\ldots)\n",
        "  + \\sum_{t=1}^\\infty \\gamma^t \\Phi(s_t)\n",
        "  - \\sum_{t=0}^\\infty \\gamma^t \\Phi(s_t)\n",
        "\\\\\n",
        "  &= R(s_0, a_0, \\ldots) - \\Phi(s_0).\n",
        "\\end{align*}\n",
        "\n",
        "It follows that\n",
        "\\begin{align*}\n",
        "  \\mathbb{E}_{\n",
        "    s_0 \\sim \\iota,\n",
        "    a_t \\sim \\pi(s_t),\n",
        "    s_{t+1} \\sim \\tau(s_t, a_t)\n",
        "  } \\left[\n",
        "    R'(s_0, a_0, \\ldots)\n",
        "  \\right]\n",
        "  &=\n",
        "  \\mathbb{E}_{\n",
        "    s_0 \\sim \\iota,\n",
        "    a_t \\sim \\pi(s_t),\n",
        "    s_{t+1} \\sim \\tau(s_t, a_t)\n",
        "  } \\left[\n",
        "    R(s_0, a_0, \\ldots) - \\Phi(s_0)\n",
        "  \\right]\n",
        "\\\\\n",
        "  &=\n",
        "  \\mathbb{E}_{\n",
        "    s_0 \\sim \\iota,\n",
        "    a_t \\sim \\pi(s_t),\n",
        "    s_{t+1} \\sim \\tau(s_t, a_t)\n",
        "  } \\left[\n",
        "    R(s_0, a_0, \\ldots)\n",
        "  \\right] -\n",
        "  \\mathbb{E}_{\n",
        "    s_0 \\sim \\iota\n",
        "  } \\left[\n",
        "    \\Phi(s_0)\n",
        "  \\right].\n",
        "\\end{align*}\n",
        "\n",
        "That is, for all policies $\\pi$, the expected return under $r$ and $r'$ differs\n",
        "by a fixed constant (independent of $\\pi$)."
      ],
      "metadata": {
        "id": "LhkwdeJ_UNkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5 solution\n",
        "---------------"
      ],
      "metadata": {
        "id": "u4_YpegMUSds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The following reward function uses a potential function that is 1 when the\n",
        "robot is holding a shard and 0 otherwise.\n",
        "\n",
        "```python\n",
        "def inventory_potential(state: State) -> float:\n",
        "    return (state.inventory == Item.SHARDS).astype(float)\n",
        "\n",
        "def reward_bin(state: State, action: Action, next_state: State) -> float:\n",
        "    return (\n",
        "        (state.bin_pos[0] == state.robot_pos[0])\n",
        "        & (state.bin_pos[1] == state.robot_pos[1])\n",
        "        & (state.inventory == Item.SHARDS)\n",
        "        & (action == Action.PUTDOWN)\n",
        "    ).astype(float)\n",
        "\n",
        "def reward_shaped(state: State, action: Action, next_state: State) -> float:\n",
        "    pickup_shaping_term = (\n",
        "        DISCOUNT_RATE * inventory_potential(next_state)\n",
        "        - inventory_potential(state)\n",
        "    )\n",
        "    bin_reward_term = reward_bin(state, action, next_state)\n",
        "    return 2 * bin_reward_term + pickup_shaping_term\n",
        "```\n",
        "\n",
        "The effect of shaping is to transform a reward function that only gives reward\n",
        "when the robot drops a shard into the bin into a reward function that gets this\n",
        "reward in advance and then loses small amounts of reward while it is holding\n",
        "the shard until it drops it into the bin, so that after discounting, the total\n",
        "return after dropping the shard into the bin is equal. If the robot drops the\n",
        "shard on the floor, it loses the initial reward that was advanced.\n"
      ],
      "metadata": {
        "id": "VhqchSM_UTyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 6 solution\n",
        "---------------\n"
      ],
      "metadata": {
        "id": "3GxlVulBUWO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We already implemented a reward function for breaking urns in task 4. Let's\n",
        "invert this into a reward function for avoiding breaking urns.\n",
        "\n",
        "```python\n",
        "def reward_no_break(state: State, action: Action, next_state: State) -> float:\n",
        "    return -2. * reward_break(state, action, next_state)\n",
        "```\n"
      ],
      "metadata": {
        "id": "rf0e0-DBUXpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 7 solution\n",
        "---------------\n"
      ],
      "metadata": {
        "id": "1SucH1uSUYzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Defining `reward2`:\n",
        "\n",
        "```python\n",
        "def reward2(state: State, action: Action, next_state: State) -> float:\n",
        "    shaped = reward_shaped(state, action, next_state)\n",
        "    nobreak = reward_no_break(state, action, next_state)\n",
        "    return shaped + nobreak\n",
        "```\n",
        "\n",
        "Training a new agent:\n",
        "```python\n",
        "key = jax.random.key(seed=42)\n",
        "key_init, key = jax.random.split(key)\n",
        "net2 = ActorCriticNetwork.init(\n",
        "    key=key_init,\n",
        "    obs_height=env.world_size,\n",
        "    obs_width=env.world_size,\n",
        "    net_channels=8,\n",
        "    net_width=16,\n",
        "    num_conv_layers=2,\n",
        "    num_dense_layers=1,\n",
        "    num_actions=len(Action),\n",
        ")\n",
        "\n",
        "key_train, key = jax.random.split(key)\n",
        "net2 = train_agent(\n",
        "    key=key_train,\n",
        "    net=net2,\n",
        "    env=env,\n",
        "    reward_fn=reward2,\n",
        "    num_train_steps=512,\n",
        ")\n",
        "```\n",
        "\n",
        "Manually inspecting behaviour:\n",
        "```python\n",
        "key_rollout = jax.random.key(seed=1)\n",
        "rollout = collect_rollout(\n",
        "    env=env,\n",
        "    key=key_rollout,\n",
        "    policy_fn=net2.policy,\n",
        "    num_steps=64,\n",
        ")\n",
        "display_rollout(env, rollout)\n",
        "```\n",
        "\n",
        "Quantitatively inspecting behaviour:\n",
        "```python\n",
        "# note: reward_bin from solution to task 5\n",
        "reward_fns = [reward2, reward_bin, reward_shaped, reward_no_break]\n",
        "return_vecs = [\n",
        "    evaluate_behaviour(\n",
        "        key=jax.random.key(seed=1),\n",
        "        env=env,\n",
        "        net=net2,\n",
        "        reward_fn=r,\n",
        "    )\n",
        "    for r in reward_fns\n",
        "]\n",
        "fig, axes = plt.subplots(len(reward_fns), figsize=(5,3*len(reward_fns)))\n",
        "for (reward_fn, returns, ax) in zip(reward_fns, return_vecs, axes):\n",
        "    ax.hist(returns)\n",
        "    ax.set_title(reward_fn.__name__)\n",
        "    ax.set_xlabel(\"return\")\n",
        "fig.tight_layout()\n",
        "fig.show()\n",
        "```"
      ],
      "metadata": {
        "id": "9FIB-eUqUaWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 8 solution\n",
        "---------------\n"
      ],
      "metadata": {
        "id": "0MdqvWWoUb73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Many altered environment layouts are permissible, here is one in which the\n",
        "shards and urns have been interchanged.\n",
        "\n",
        "```python\n",
        "env2 = Environment(\n",
        "    init_robot_pos=jnp.array((3,4), dtype=jnp.uint8),\n",
        "    init_items_map=jnp.array((\n",
        "      (0,0,0,0,1,1),\n",
        "      (0,2,0,0,0,1),\n",
        "      (0,0,0,0,0,0),\n",
        "      (0,2,2,0,0,1),\n",
        "      (0,0,0,0,1,1),\n",
        "      (1,0,2,0,1,1),\n",
        "    ), dtype=jnp.uint8),\n",
        "    bin_pos=jnp.array((0,0), dtype=jnp.uint8),\n",
        ")\n",
        "```\n",
        "\n",
        "We can inspect the behaviour of `net2` on this environment as follows.\n",
        "```python\n",
        "key_rollout = jax.random.key(seed=1)\n",
        "rollout = collect_rollout(\n",
        "    env=env2,\n",
        "    key=key_rollout,\n",
        "    policy_fn=net2.policy,\n",
        "    num_steps=64,\n",
        ")\n",
        "display_rollout(env2, rollout)\n",
        "```"
      ],
      "metadata": {
        "id": "5ax9yK8oUdPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 9 solution\n",
        "---------------\n",
        "\n",
        "See the opening of part 5."
      ],
      "metadata": {
        "id": "Sk-0YUSUUfqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 10 solution\n",
        "----------------\n"
      ],
      "metadata": {
        "id": "8wrr0jn_UkMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For sub-task 1, there are many possible environments. Here is one in which the\n",
        "bin is in the top right corner instead of the top left corner.\n",
        "\n",
        "```python\n",
        "env_shift = Environment(\n",
        "    init_robot_pos=jnp.array((2,2), dtype=jnp.uint8),\n",
        "    init_items_map=jnp.array((\n",
        "      (0,0,0,0),\n",
        "      (0,0,1,0),\n",
        "      (0,1,0,2),\n",
        "      (0,0,2,0),\n",
        "    ), dtype=jnp.uint8),\n",
        "    bin_pos=jnp.array((0,3), dtype=jnp.uint8),\n",
        ")\n",
        "```\n",
        "\n",
        "The behavioural objective is to drop shards in the top left corner.\n",
        "\n",
        "```\n",
        "def proxy(state: State, action: Action, next_state: State) -> float:\n",
        "    item_below_robot = state.items_map[\n",
        "        state.robot_pos[0],\n",
        "        state.robot_pos[1],\n",
        "    ]\n",
        "    return (\n",
        "        (state.robot_pos[0] == 0)\n",
        "        & (state.robot_pos[1] == 0)\n",
        "        & (item_below_robot == Item.EMPTY)\n",
        "        & (state.inventory == Item.SHARDS)\n",
        "        & (action == Action.PUTDOWN)\n",
        "    ).astype(float)\n",
        "```\n",
        "\n",
        "The evaluation code can be adapted from 'quantifying reward hacking' after task 4.\n",
        "\n",
        "```python\n",
        "reward_fns = [reward2, proxy]\n",
        "return_vecs = [\n",
        "    evaluate_behaviour(\n",
        "        key=jax.random.key(seed=1),\n",
        "        env=env_shift,\n",
        "        net=net3,\n",
        "        reward_fn=r,\n",
        "    )\n",
        "    for r in reward_fns\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(len(reward_fns), figsize=(5,3*len(reward_fns)))\n",
        "for (reward_fn, returns, ax) in zip(reward_fns, return_vecs, axes):\n",
        "    ax.hist(returns)\n",
        "    ax.set_title(reward_fn.__name__)\n",
        "    ax.set_xlabel(\"return\")\n",
        "fig.tight_layout()\n",
        "fig.show()\n",
        "```"
      ],
      "metadata": {
        "id": "6eYo0UJZUlp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 11 solution\n",
        "----------------\n"
      ],
      "metadata": {
        "id": "rdCvmSmnUoNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "An easy way to do this is to incorporate the bin placement into the same\n",
        "`jax.random.choice` call as the robot and the other items.\n",
        "\n",
        "```python\n",
        "def generate_shift(\n",
        "    key: PRNGKeyArray,\n",
        "    world_size: int,\n",
        "    num_shards: int,\n",
        "    num_urns: int,\n",
        ") -> Environment:\n",
        "    # list of possible item/robot coordinates\n",
        "    coords = einops.rearrange(\n",
        "        jnp.indices((world_size, world_size), dtype=jnp.uint8),\n",
        "        'c h w -> c (h w)',\n",
        "    )\n",
        "\n",
        "    # sample bin, robot and item positions without replacement\n",
        "    num_positions = 1 + 1 + num_shards + num_urns\n",
        "    all_positions = jax.random.choice(\n",
        "        key=key,\n",
        "        a=coords,\n",
        "        shape=(num_positions,),\n",
        "        axis=1,\n",
        "        replace=False,\n",
        "    )\n",
        "    bin_pos = all_positions[:, 0]\n",
        "    robot_pos = all_positions[:, 1]\n",
        "    items_pos = all_positions[:, 2:]\n",
        "\n",
        "    # create item map\n",
        "    items_map = jnp.zeros((world_size, world_size), dtype=jnp.uint8)\n",
        "    items_map = items_map.at[\n",
        "        items_pos[0, :num_shards],\n",
        "        items_pos[1, :num_shards],\n",
        "    ].set(Item.SHARDS)\n",
        "    items_map = items_map.at[\n",
        "        items_pos[0, num_shards:],\n",
        "        items_pos[1, num_shards:],\n",
        "    ].set(Item.URN)\n",
        "    \n",
        "    return Environment(\n",
        "        init_robot_pos=robot_pos,\n",
        "        init_items_map=items_map,\n",
        "        bin_pos=bin_pos,\n",
        "    )\n",
        "```"
      ],
      "metadata": {
        "id": "euKhiJ4AUpIa"
      }
    }
  ]
}